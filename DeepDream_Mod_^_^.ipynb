{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pri-Mohanty/DeepDream_Mod/blob/main/DeepDream_Mod_%5E_%5E.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VHtOGjQ5XSYq"
      },
      "source": [
        "# **Original DeepDream Algorithm**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OSACju--QAlI"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import PIL.Image\n",
        "import urllib.request\n",
        "from moviepy.editor import ImageSequenceClip\n",
        "import glob\n",
        "\n",
        "\n",
        "# Load a pre-trained InceptionV3 model from TensorFlow Hub\n",
        "base_model = tf.keras.applications.InceptionV3(include_top=False, weights='imagenet')\n",
        "base_model.trainable = False\n",
        "\n",
        "# Define a list of layers to target for feature visualization\n",
        "layer_names = ['mixed3', 'mixed5', 'mixed7']\n",
        "\n",
        "# Define the model that outputs the activation values for the target layers\n",
        "dream_model = tf.keras.Model(inputs=base_model.input, outputs=[base_model.get_layer(name).output for name in layer_names])\n",
        "\n",
        "# Define the target layer index for DeepDream\n",
        "target_layer_index = layer_names.index('mixed5')\n",
        "\n",
        "def calc_loss(img, model):\n",
        "    # Calculate the activations of the target layers\n",
        "    img_batch = tf.expand_dims(img, axis=0)\n",
        "    layer_activations = model(img_batch)\n",
        "\n",
        "    losses = []\n",
        "    for act in layer_activations:\n",
        "        # For each activation, maximize the mean of the activation values\n",
        "        loss = tf.math.reduce_mean(act)\n",
        "        losses.append(loss)\n",
        "\n",
        "    return tf.reduce_sum(losses)\n",
        "\n",
        "def calculate_cam(model, img, target_layer_index):\n",
        "    \"\"\"Calculates Class Activation Mapping (CAM).\"\"\"\n",
        "    # Convert the image to a tensor if it is a numpy array\n",
        "    if not tf.is_tensor(img):\n",
        "        img = tf.convert_to_tensor(img, dtype=tf.float32)\n",
        "    with tf.GradientTape() as tape:\n",
        "        tape.watch(img)\n",
        "        layer_activations = model(img)\n",
        "        target_activation = layer_activations[target_layer_index]\n",
        "        loss = tf.reduce_mean(target_activation)\n",
        "    grads = tape.gradient(loss, target_activation)\n",
        "    weights = tf.reduce_mean(grads, axis=(0, 1, 2))  # Global average pooling\n",
        "    cam = tf.reduce_sum(tf.multiply(weights, target_activation), axis=-1)\n",
        "    cam = tf.nn.relu(cam)  # ReLU activation\n",
        "    cam = cam / tf.reduce_max(cam)  # Normalize to [0, 1]\n",
        "    return cam.numpy()\n",
        "\n",
        "def visualize_cam(cam, img, alpha=0.5):\n",
        "    \"\"\"Overlays CAM onto the original image.\"\"\"\n",
        "    # Ensure cam is 2D\n",
        "    if len(cam.shape) > 2:\n",
        "        cam = cam.squeeze()\n",
        "    # Resize CAM to match the original image dimensions\n",
        "    cam = cv2.resize(cam, (img.shape[1], img.shape[0]))\n",
        "    # Normalize CAM to [0, 255] and ensure values are within range\n",
        "    cam = np.uint8(np.clip(cam * 255, 0, 255))\n",
        "    # Convert CAM to heatmap\n",
        "    heatmap = cv2.applyColorMap(cam, cv2.COLORMAP_JET)\n",
        "    # Overlay heatmap on the original image\n",
        "    heatmap = np.float32(heatmap) / 255\n",
        "    cam_img = heatmap * alpha + img\n",
        "    # Normalize the overlay image\n",
        "    cam_img = cam_img / np.max(cam_img)\n",
        "    return cam_img\n",
        "\n",
        "\n",
        "def calculate_deepdream_score(model, img, target_layer_index):\n",
        "    \"\"\"Calculates DeepDream Score based on activation strength.\"\"\"\n",
        "    layer_activations = model(img)\n",
        "    target_activation = layer_activations[target_layer_index]\n",
        "    score = tf.reduce_mean(tf.abs(target_activation)).numpy()  # You can customize this calculation\n",
        "    return score\n",
        "\n",
        "def deepdream(img, model, steps=100, step_size=0.01):\n",
        "    for step in range(steps):\n",
        "        # Calculate the gradients of the image with respect to the loss\n",
        "        with tf.GradientTape() as tape:\n",
        "            tape.watch(img)\n",
        "            loss = calc_loss(img, model)\n",
        "\n",
        "        gradients = tape.gradient(loss, img)\n",
        "\n",
        "        # Normalize the gradients\n",
        "        gradients /= tf.math.reduce_std(gradients) + 1e-8\n",
        "\n",
        "        # Update the image using gradient ascent\n",
        "        img = img + gradients * step_size\n",
        "        img = tf.clip_by_value(img, -1, 1)\n",
        "\n",
        "        print(f\"Step {step}: Gradients mean: {np.mean(gradients)}, Gradients max: {np.max(gradients)}\")\n",
        "        print(f\"Step {step}: Loss: {loss}\")\n",
        "\n",
        "        # Capture CAM at intermediate steps\n",
        "        if step % 2 == 0:\n",
        "            # Convert img to NumPy array before visualizing\n",
        "            img_np = img.numpy()\n",
        "            cam = calculate_cam(model, np.expand_dims(img, axis=0), target_layer_index)\n",
        "            cam_img = visualize_cam(cam, img_np.squeeze())\n",
        "            cam_img = (cam_img * 255).astype(np.uint8)\n",
        "            cam_img = Image.fromarray(cam_img)\n",
        "            cam_img.save(f\"cam_output_{step}.png\")\n",
        "            print(f\"CAM visualization saved to cam_output_{step}.png\")\n",
        "\n",
        "    return img.numpy()\n",
        "\n",
        "    return img\n",
        "\n",
        "def run_deepdream(image_path, steps=100, step_size=0.01):\n",
        "    # Load the input image\n",
        "    img = tf.keras.preprocessing.image.load_img(image_path)\n",
        "    img = tf.keras.preprocessing.image.img_to_array(img)\n",
        "    img = tf.keras.applications.inception_v3.preprocess_input(img)\n",
        "\n",
        "    # Convert to a tensor and run DeepDream\n",
        "    img = tf.convert_to_tensor(img)\n",
        "    img = deepdream(img, dream_model, steps, step_size)\n",
        "\n",
        "    # Post-process the output image\n",
        "    img = tf.keras.preprocessing.image.array_to_img(img)\n",
        "    img.save('deepdream_output.jpg')\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    image_url = \"https://i0.wp.com/picjumbo.com/wp-content/uploads/man-looking-into-the-distance-on-top-of-the-mountain-free-photo.jpg?w=600&quality=80\"\n",
        "    urllib.request.urlretrieve(image_url, \"input_image.jpg\")\n",
        "    run_deepdream(\"input_image.jpg\", steps=50, step_size=0.01)\n",
        "    image_files = sorted(glob.glob('cam_output_*.png'))\n",
        "    clip = ImageSequenceClip(image_files, fps=15)\n",
        "    clip.write_videofile(\"deepdream_cam_video.mp4\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ourMke7l38RC"
      },
      "source": [
        "# **DeepDream with Guided BackProp**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "vVxJ95-N0fGD",
        "outputId": "07e244eb-7471-4cdf-9267-ac08e2067fb9",
        "collapsed": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.15.0)\n",
            "Collecting tensorflow\n",
            "  Downloading tensorflow-2.16.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (589.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m589.8/589.8 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Collecting h5py>=3.10.0 (from tensorflow)\n",
            "  Downloading h5py-3.11.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.3/5.3 MB\u001b[0m \u001b[31m45.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Collecting ml-dtypes~=0.3.1 (from tensorflow)\n",
            "  Downloading ml_dtypes-0.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m57.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.31.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.11.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.63.0)\n",
            "Collecting tensorboard<2.17,>=2.16 (from tensorflow)\n",
            "  Downloading tensorboard-2.16.2-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m52.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting keras>=3.0.0 (from tensorflow)\n",
            "  Downloading keras-3.3.3-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m66.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.0)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.25.2)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.43.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.0.0->tensorflow) (13.7.1)\n",
            "Collecting namex (from keras>=3.0.0->tensorflow)\n",
            "  Downloading namex-0.0.8-py3-none-any.whl (5.8 kB)\n",
            "Collecting optree (from keras>=3.0.0->tensorflow)\n",
            "  Downloading optree-0.11.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (311 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.2/311.2 kB\u001b[0m \u001b[31m31.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2024.2.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.17,>=2.16->tensorflow) (3.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.17,>=2.16->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.17,>=2.16->tensorflow) (3.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow) (2.1.5)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.0.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.0.0->tensorflow) (2.16.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.0.0->tensorflow) (0.1.2)\n",
            "Installing collected packages: namex, optree, ml-dtypes, h5py, tensorboard, keras, tensorflow\n",
            "  Attempting uninstall: ml-dtypes\n",
            "    Found existing installation: ml-dtypes 0.2.0\n",
            "    Uninstalling ml-dtypes-0.2.0:\n",
            "      Successfully uninstalled ml-dtypes-0.2.0\n",
            "  Attempting uninstall: h5py\n",
            "    Found existing installation: h5py 3.9.0\n",
            "    Uninstalling h5py-3.9.0:\n",
            "      Successfully uninstalled h5py-3.9.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.15.2\n",
            "    Uninstalling tensorboard-2.15.2:\n",
            "      Successfully uninstalled tensorboard-2.15.2\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.15.0\n",
            "    Uninstalling keras-2.15.0:\n",
            "      Successfully uninstalled keras-2.15.0\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.15.0\n",
            "    Uninstalling tensorflow-2.15.0:\n",
            "      Successfully uninstalled tensorflow-2.15.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tf-keras 2.15.1 requires tensorflow<2.16,>=2.15, but you have tensorflow 2.16.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed h5py-3.11.0 keras-3.3.3 ml-dtypes-0.3.2 namex-0.0.8 optree-0.11.0 tensorboard-2.16.2 tensorflow-2.16.1\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "id": "7e62a8df3546415fbac693b087162957",
              "pip_warning": {
                "packages": [
                  "h5py",
                  "keras",
                  "ml_dtypes",
                  "tensorboard",
                  "tensorflow"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "!pip install --upgrade tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "70eJBz6D4ISG",
        "outputId": "fa0f6c98-9c64-4e4a-a08c-6d17de83f5d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 0: Gradients mean: 4.780577267382569e-09, Gradients max: 0.001466822810471058\n",
            "Step 0: Loss: 0.04755181819200516\n",
            "CAM visualization saved to cam_output_0.png\n",
            "Step 1: Gradients mean: 4.7738839548117085e-09, Gradients max: 0.0014655559789389372\n",
            "Step 1: Loss: 0.04755200445652008\n",
            "Step 2: Gradients mean: 4.7798875968396715e-09, Gradients max: 0.0014648305950686336\n",
            "Step 2: Loss: 0.047552190721035004\n",
            "CAM visualization saved to cam_output_2.png\n",
            "Step 3: Gradients mean: 4.777386486409796e-09, Gradients max: 0.0014646705240011215\n",
            "Step 3: Loss: 0.047552380710840225\n",
            "DeepDream image saved to deepdream_output.jpg\n",
            "Moviepy - Building video deepdream_cam_video.mp4.\n",
            "Moviepy - Writing video deepdream_cam_video.mp4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "                                                  "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready deepdream_cam_video.mp4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import requests\n",
        "import io\n",
        "from google.colab import files\n",
        "from moviepy.editor import ImageSequenceClip\n",
        "import glob\n",
        "\n",
        "# Load a pre-trained InceptionV3 model from TensorFlow Hub\n",
        "base_model = tf.keras.applications.InceptionV3(include_top=False, weights='imagenet')\n",
        "base_model.trainable = False\n",
        "\n",
        "# Define a list of layers to target for feature visualization\n",
        "layer_names = ['mixed3', 'mixed5', 'mixed7']\n",
        "\n",
        "# Define the model that outputs the activation values for the target layers\n",
        "dream_model = tf.keras.Model(inputs=base_model.input, outputs=[base_model.get_layer(name).output for name in layer_names])\n",
        "\n",
        "# Define the target layer index for DeepDream\n",
        "target_layer_index = layer_names.index('mixed7')\n",
        "\n",
        "def guided_backprop(model, image, target_layer_index):\n",
        "    with tf.GradientTape() as tape:\n",
        "        tape.watch(image)\n",
        "        layer_activations = model(image)\n",
        "        target_activation = layer_activations[target_layer_index]\n",
        "        loss = tf.math.reduce_mean(target_activation)\n",
        "\n",
        "    grads = tape.gradient(loss, image)\n",
        "    return grads\n",
        "\n",
        "\n",
        "def deepdream(img, model, steps, step_size):\n",
        "    img = tf.convert_to_tensor(img)\n",
        "    for step in range(steps):\n",
        "        grads = guided_backprop(model, img, target_layer_index)\n",
        "        loss = tf.math.reduce_mean(model(img)[target_layer_index])\n",
        "        grads = tf.clip_by_value(grads, -1, 1)  # Clip gradients to [-1, 1]\n",
        "        img = tf.clip_by_value(img + grads * step_size, -1, 1)\n",
        "        print(f\"Step {step}: Gradients mean: {np.mean(grads)}, Gradients max: {np.max(grads)}\")\n",
        "        print(f\"Step {step}: Loss: {loss}\")\n",
        "\n",
        "        # Capture CAM at intermediate steps\n",
        "        if step % 2 == 0:\n",
        "            # Convert img to NumPy array before visualizing\n",
        "            img_np = img.numpy()\n",
        "            cam = calculate_cam(model, img, target_layer_index)\n",
        "            cam_img = visualize_cam(cam, img_np.squeeze())\n",
        "            cam_img = (cam_img * 255).astype(np.uint8)\n",
        "            cam_img = Image.fromarray(cam_img)\n",
        "            cam_img.save(f\"cam_output_{step}.png\")\n",
        "            print(f\"CAM visualization saved to cam_output_{step}.png\")\n",
        "\n",
        "    return img.numpy()\n",
        "\n",
        "def calculate_cam(model, img, target_layer_index):\n",
        "    \"\"\"Calculates Class Activation Mapping (CAM).\"\"\"\n",
        "    # Convert the image to a tensor if it is a numpy array\n",
        "    if not tf.is_tensor(img):\n",
        "        img = tf.convert_to_tensor(img, dtype=tf.float32)\n",
        "    with tf.GradientTape() as tape:\n",
        "        tape.watch(img)\n",
        "        layer_activations = model(img)\n",
        "        target_activation = layer_activations[target_layer_index]\n",
        "        loss = tf.reduce_mean(target_activation)\n",
        "    grads = tape.gradient(loss, target_activation)\n",
        "    weights = tf.reduce_mean(grads, axis=(0, 1, 2))  # Global average pooling\n",
        "    cam = tf.reduce_sum(tf.multiply(weights, target_activation), axis=-1)\n",
        "    cam = tf.nn.relu(cam)  # ReLU activation\n",
        "    cam = cam / tf.reduce_max(cam)  # Normalize to [0, 1]\n",
        "    return cam.numpy()\n",
        "\n",
        "def visualize_cam(cam, img, alpha=0.5):\n",
        "    \"\"\"Overlays CAM onto the original image.\"\"\"\n",
        "    # Ensure cam is 2D\n",
        "    if len(cam.shape) > 2:\n",
        "        cam = cam.squeeze()\n",
        "    # Resize CAM to match the original image dimensions\n",
        "    cam = cv2.resize(cam, (img.shape[1], img.shape[0]))\n",
        "    # Normalize CAM to [0, 255] and ensure values are within range\n",
        "    cam = np.uint8(np.clip(cam * 255, 0, 255))\n",
        "    # Convert CAM to heatmap\n",
        "    heatmap = cv2.applyColorMap(cam, cv2.COLORMAP_JET)\n",
        "    # Overlay heatmap on the original image\n",
        "    heatmap = np.float32(heatmap) / 255\n",
        "    cam_img = heatmap * alpha + img\n",
        "    # Normalize the overlay image\n",
        "    cam_img = cam_img / np.max(cam_img)\n",
        "    return cam_img\n",
        "\n",
        "def calculate_sparsity(activations, threshold=0.1):\n",
        "    \"\"\"Calculates activation sparsity.\"\"\"\n",
        "    active_neurons = tf.cast(activations > threshold, tf.float32)\n",
        "    sparsity = 1.0 - tf.reduce_mean(active_neurons).numpy()\n",
        "    return sparsity\n",
        "\n",
        "def calculate_deepdream_score(model, img, target_layer_index):\n",
        "    \"\"\"Calculates DeepDream Score based on activation strength.\"\"\"\n",
        "    layer_activations = model(img)\n",
        "    target_activation = layer_activations[target_layer_index]\n",
        "    score = tf.reduce_mean(tf.abs(target_activation)).numpy()  # You can customize this calculation\n",
        "    return score\n",
        "\n",
        "\n",
        "def download_and_preprocess_image(image_url):\n",
        "    \"\"\"Downloads an image from the URL and preprocesses it for DeepDream.\"\"\"\n",
        "    response = requests.get(image_url, stream=True)\n",
        "    response.raise_for_status()\n",
        "    # Decode the image data using Pillow\n",
        "    img = Image.open(response.raw)\n",
        "    img = img.convert('RGB')  # Ensure RGB format\n",
        "    # Convert to numpy array and resize (optional)\n",
        "    img = np.array(img)\n",
        "    img = np.asarray(img, dtype='float32')\n",
        "    # Manual Preprocessing (replace with your desired steps)\n",
        "    img = img / 255.0  # Normalize pixel values to [0, 1]\n",
        "    img = np.expand_dims(img, axis=0)  # Add batch dimension\n",
        "    return img\n",
        "\n",
        "def run_deepdream(image_url, steps=50, step_size=0.0001):\n",
        "    img = download_and_preprocess_image(image_url)\n",
        "    dream_img = deepdream(img, dream_model, steps=steps, step_size=step_size)\n",
        "    dream_img = np.squeeze(dream_img, axis=0)  # Remove batch dimension\n",
        "    dream_img = np.clip(dream_img, 0, 1)  # Clip values to [0, 1] range\n",
        "    dream_img = (dream_img * 255).astype(np.uint8)  # Convert to uint8\n",
        "    dream_img = Image.fromarray(dream_img)\n",
        "\n",
        "    # Save the image to a file\n",
        "    output_path = \"deepdream_output.jpg\"\n",
        "    dream_img.save(output_path, format='JPEG')\n",
        "    print(f\"DeepDream image saved to {output_path}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    image_url = \"https://i0.wp.com/picjumbo.com/wp-content/uploads/man-looking-into-the-distance-on-top-of-the-mountain-free-photo.jpg?w=600&quality=80\"\n",
        "    run_deepdream(image_url, steps=4, step_size=0.0001)\n",
        "    image_files = sorted(glob.glob('cam_output_*.png'))\n",
        "    clip = ImageSequenceClip(image_files, fps=15)\n",
        "    clip.write_videofile(\"deepdream_cam_video.mp4\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_zPsRY4W4VpQ"
      },
      "source": [
        "**Above code gave unremakable outputs.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "12gc4M9CFZxS"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rmKC2BH3mXWn"
      },
      "source": [
        "# **Adding total_variation_regularization**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6hwV2Y5xGrcL",
        "outputId": "44decd27-2970-429c-bf3a-fa2ac13fdeca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:py.warnings:<ipython-input-34-f3d586882a90>:76: DeprecationWarning: ANTIALIAS is deprecated and will be removed in Pillow 10 (2023-07-01). Use LANCZOS or Resampling.LANCZOS instead.\n",
            "  img = img.resize((299, 299), Image.ANTIALIAS)  # Resize to 299x299 pixels using ANTIALIAS resampling\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1, 299, 299, 3)\n",
            "Input image min: 0.0, Input image max: 1.0\n",
            "Step 0: Gradients mean: -6.127841345460183e-08, Gradients max: 0.0024878198746591806\n",
            "Step 0: Loss: 0.4330541789531708\n",
            "CAM visualization saved to cam_output_0.png\n",
            "Step 1: Gradients mean: -6.302420985093704e-08, Gradients max: 0.0019795973785221577\n",
            "Step 1: Loss: 0.4370095729827881\n",
            "Step 2: Gradients mean: -6.564793864072271e-08, Gradients max: 0.0015474032843485475\n",
            "Step 2: Loss: 0.4409131705760956\n",
            "CAM visualization saved to cam_output_2.png\n",
            "Step 3: Gradients mean: -6.687076137268377e-08, Gradients max: 0.0015458071138709784\n",
            "Step 3: Loss: 0.44468024373054504\n",
            "Step 4: Gradients mean: -6.843161770575534e-08, Gradients max: 0.0016221324913203716\n",
            "Step 4: Loss: 0.448424369096756\n",
            "CAM visualization saved to cam_output_4.png\n",
            "Step 5: Gradients mean: -6.979712452448439e-08, Gradients max: 0.0015874376986175776\n",
            "Step 5: Loss: 0.4521368443965912\n",
            "Step 6: Gradients mean: -6.994440582275274e-08, Gradients max: 0.0016415618592873216\n",
            "Step 6: Loss: 0.4557970464229584\n",
            "CAM visualization saved to cam_output_6.png\n",
            "Step 7: Gradients mean: -6.981071010159212e-08, Gradients max: 0.0016970708966255188\n",
            "Step 7: Loss: 0.45941346883773804\n",
            "Step 8: Gradients mean: -7.289754933026416e-08, Gradients max: 0.0015725193079560995\n",
            "Step 8: Loss: 0.4630158543586731\n",
            "CAM visualization saved to cam_output_8.png\n",
            "Step 9: Gradients mean: -7.390422496200699e-08, Gradients max: 0.001376550062559545\n",
            "Step 9: Loss: 0.46657636761665344\n",
            "DeepDream Score: 0.42622244358062744\n",
            "DeepDream image saved to deepdream_output.png\n",
            "CAM visualization saved to final_cam_output.png\n",
            "Original sparsity: 0.5396076738834381, DeepDream sparsity: 0.6130514740943909\n",
            "Moviepy - Building video deepdream_cam_video.mp4.\n",
            "Moviepy - Writing video deepdream_cam_video.mp4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready deepdream_cam_video.mp4\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import requests\n",
        "import urllib.request\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "from moviepy.editor import ImageSequenceClip\n",
        "import glob\n",
        "\n",
        "# Load a pre-trained InceptionV3 model from TensorFlow Hub\n",
        "base_model = tf.keras.applications.InceptionV3(include_top=False, weights='imagenet')\n",
        "base_model.trainable = False\n",
        "\n",
        "# Define a list of layers to target for feature visualization\n",
        "layer_names = ['mixed3', 'mixed5', 'mixed7']\n",
        "\n",
        "# Define the model that outputs the activation values for the target layers\n",
        "dream_model = tf.keras.Model(inputs=base_model.input, outputs=[base_model.get_layer(name).output for name in layer_names])\n",
        "\n",
        "# Define the target layer index for DeepDream\n",
        "target_layer_index = layer_names.index('mixed3')  # Change the target layer here if desired\n",
        "\n",
        "# Define the regularization strength for total variation regularization\n",
        "tv_reg_strength = 1e-6  # Adjust this value to control the smoothness\n",
        "\n",
        "def guided_backprop(model, image, target_layer_index):\n",
        "    with tf.GradientTape() as tape:\n",
        "        tape.watch(image)\n",
        "        layer_activations = model(image)\n",
        "        target_activation = layer_activations[target_layer_index]\n",
        "        loss = tf.math.reduce_mean(target_activation)\n",
        "\n",
        "    grads = tape.gradient(loss, image)\n",
        "    return grads\n",
        "\n",
        "def deepdream(img, model, steps, step_size):\n",
        "    img = tf.convert_to_tensor(img)\n",
        "    for step in range(steps):\n",
        "        grads = guided_backprop(model, img, target_layer_index)\n",
        "        tv_loss = total_variation_regularization(img)\n",
        "        loss = tf.math.reduce_mean(model(img)[target_layer_index]) + tv_reg_strength * tv_loss\n",
        "        grads = tf.clip_by_value(grads, -1, 1)  # Clip gradients to [-1, 1]\n",
        "        img = tf.clip_by_value(img + grads * step_size, -1, 1)\n",
        "        print(f\"Step {step}: Gradients mean: {np.mean(grads)}, Gradients max: {np.max(grads)}\")\n",
        "        print(f\"Step {step}: Loss: {loss}\")\n",
        "\n",
        "        # Capture CAM at intermediate steps\n",
        "        if step % 2 == 0:\n",
        "            # Convert img to NumPy array before visualizing\n",
        "            img_np = img.numpy()\n",
        "            cam = calculate_cam(model, img, target_layer_index)\n",
        "            cam_img = visualize_cam(cam, img_np.squeeze())\n",
        "            cam_img = (cam_img * 255).astype(np.uint8)\n",
        "            cam_img = Image.fromarray(cam_img)\n",
        "            cam_img.save(f\"cam_output_{step}.png\")\n",
        "            print(f\"CAM visualization saved to cam_output_{step}.png\")\n",
        "\n",
        "    return img.numpy()\n",
        "\n",
        "\n",
        "def total_variation_regularization(img):\n",
        "    \"\"\"Computes the total variation regularization loss for an image.\"\"\"\n",
        "    x_diff = img[:, :, :-1, :] - img[:, :, 1:, :]\n",
        "    y_diff = img[:, :-1, :, :] - img[:, 1:, :, :]\n",
        "    return tf.reduce_sum(tf.abs(x_diff)) + tf.reduce_sum(tf.abs(y_diff))\n",
        "\n",
        "def download_and_preprocess_image(image_url):\n",
        "    \"\"\"Downloads an image from the URL and preprocesses it for DeepDream.\"\"\"\n",
        "    response = requests.get(image_url, stream=True)\n",
        "    response.raise_for_status()\n",
        "    # Decode the image data using Pillow\n",
        "    img = Image.open(response.raw)\n",
        "    img = img.convert('RGB')  # Ensure RGB format\n",
        "    # Resize the image to the expected input size\n",
        "    img = img.resize((299, 299), Image.ANTIALIAS)  # Resize to 299x299 pixels using ANTIALIAS resampling\n",
        "    # Convert to numpy array\n",
        "    img = np.array(img)\n",
        "    img = np.asarray(img, dtype='float32')\n",
        "    # Manual Preprocessing (replace with your desired steps)\n",
        "    img = img / 255.0  # Normalize pixel values to [0, 1]\n",
        "    img = np.expand_dims(img, axis=0)  # Add batch dimension\n",
        "    print(img.shape)\n",
        "    return img\n",
        "\n",
        "def calculate_cam(model, img, target_layer_index):\n",
        "    \"\"\"Calculates Class Activation Mapping (CAM).\"\"\"\n",
        "    # Convert the image to a tensor if it is a numpy array\n",
        "    if not tf.is_tensor(img):\n",
        "        img = tf.convert_to_tensor(img, dtype=tf.float32)\n",
        "    with tf.GradientTape() as tape:\n",
        "        tape.watch(img)\n",
        "        layer_activations = model(img)\n",
        "        target_activation = layer_activations[target_layer_index]\n",
        "        loss = tf.reduce_mean(target_activation)\n",
        "    grads = tape.gradient(loss, target_activation)\n",
        "    weights = tf.reduce_mean(grads, axis=(0, 1, 2))  # Global average pooling\n",
        "    cam = tf.reduce_sum(tf.multiply(weights, target_activation), axis=-1)\n",
        "    cam = tf.nn.relu(cam)  # ReLU activation\n",
        "    cam = cam / tf.reduce_max(cam)  # Normalize to [0, 1]\n",
        "    return cam.numpy()\n",
        "\n",
        "def visualize_cam(cam, img, alpha=0.5):\n",
        "    \"\"\"Overlays CAM onto the original image.\"\"\"\n",
        "    # Ensure cam is 2D\n",
        "    if len(cam.shape) > 2:\n",
        "        cam = cam.squeeze()\n",
        "    # Resize CAM to match the original image dimensions\n",
        "    cam = cv2.resize(cam, (img.shape[1], img.shape[0]))\n",
        "    # Normalize CAM to [0, 255] and ensure values are within range\n",
        "    cam = np.uint8(np.clip(cam * 255, 0, 255))\n",
        "    # Convert CAM to heatmap\n",
        "    heatmap = cv2.applyColorMap(cam, cv2.COLORMAP_JET)\n",
        "    # Overlay heatmap on the original image\n",
        "    heatmap = np.float32(heatmap) / 255\n",
        "    cam_img = heatmap * alpha + img\n",
        "    # Normalize the overlay image\n",
        "    cam_img = cam_img / np.max(cam_img)\n",
        "    return cam_img\n",
        "\n",
        "def calculate_sparsity(activations, threshold=0.1):\n",
        "    \"\"\"Calculates activation sparsity.\"\"\"\n",
        "    active_neurons = tf.cast(activations > threshold, tf.float32)\n",
        "    sparsity = 1.0 - tf.reduce_mean(active_neurons).numpy()\n",
        "    return sparsity\n",
        "\n",
        "def calculate_deepdream_score(model, img, target_layer_index):\n",
        "    \"\"\"Calculates DeepDream Score based on activation strength.\"\"\"\n",
        "    layer_activations = model(img)\n",
        "    target_activation = layer_activations[target_layer_index]\n",
        "    score = tf.reduce_mean(tf.abs(target_activation)).numpy()\n",
        "    return score\n",
        "\n",
        "def run_deepdream(image_url, steps, step_size):\n",
        "    img = download_and_preprocess_image(image_url)\n",
        "    print(f\"Input image min: {np.min(img)}, Input image max: {np.max(img)}\")\n",
        "    dream_img = deepdream(img, dream_model, steps=steps, step_size=step_size)\n",
        "    dream_img = np.squeeze(dream_img, axis=0)  # Remove batch dimension\n",
        "    dream_img = np.clip(dream_img, 0, 1)  # Clip values to [0, 1] range\n",
        "    dream_img = (dream_img * 255).astype(np.uint8)  # Convert to uint8\n",
        "    dream_img = Image.fromarray(dream_img)\n",
        "    cam = calculate_cam(dream_model, img, target_layer_index)\n",
        "    deepdream_score = calculate_deepdream_score(dream_model, img, target_layer_index)\n",
        "    print(f\"DeepDream Score: {deepdream_score}\")\n",
        "    # Save the image to a file\n",
        "    output_path = \"deepdream_output.png\"\n",
        "    dream_img.save(output_path, format='PNG')\n",
        "    print(f\"DeepDream image saved to {output_path}\")\n",
        "    # Visualize and save the CAM\n",
        "    cam_img = visualize_cam(cam, img.squeeze())  # Remove batch dimension from img\n",
        "    cam_img = (cam_img * 255).astype(np.uint8)\n",
        "    cam_img = Image.fromarray(cam_img)\n",
        "    cam_img.save(\"final_cam_output.png\")\n",
        "    print(\"CAM visualization saved to final_cam_output.png\")\n",
        "    original_activations = dream_model(img)[target_layer_index]\n",
        "\n",
        "    # Add back the batch dimension to dream_img for the model to work\n",
        "    dream_img = tf.expand_dims(dream_img, axis=0)\n",
        "    dream_activations = dream_model(tf.convert_to_tensor(dream_img))[target_layer_index]\n",
        "    original_sparsity = calculate_sparsity(original_activations)\n",
        "    dream_sparsity = calculate_sparsity(dream_activations)\n",
        "    print(f\"Original sparsity: {original_sparsity}, DeepDream sparsity: {dream_sparsity}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  image_url = \"https://i0.wp.com/picjumbo.com/wp-content/uploads/man-looking-into-the-distance-on-top-of-the-mountain-free-photo.jpg?w=600&quality=80\"\n",
        "  urllib.request.urlretrieve(image_url, \"input_image.jpg\")\n",
        "  run_deepdream(image_url, steps=10, step_size=1.0)\n",
        "  target_size = (299, 299)\n",
        "  image_files = sorted(glob.glob('cam_output_*.png'))\n",
        "  for image_file in image_files:\n",
        "    img = Image.open(image_file)\n",
        "    img = img.resize(target_size, Image.LANCZOS)\n",
        "    img.save(image_file)\n",
        "  clip = ImageSequenceClip(image_files, fps=15)\n",
        "  clip.write_videofile(\"deepdream_cam_video.mp4\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Final Implementation**"
      ],
      "metadata": {
        "id": "fNd8Q40aArLj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install shap"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "g6NNQjOoKvX_",
        "outputId": "af521bf6-6e86-4d53-bfab-1b8271af12aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: shap in /usr/local/lib/python3.10/dist-packages (0.46.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from shap) (1.26.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from shap) (1.13.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from shap) (1.5.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from shap) (2.2.2)\n",
            "Requirement already satisfied: tqdm>=4.27.0 in /usr/local/lib/python3.10/dist-packages (from shap) (4.66.5)\n",
            "Requirement already satisfied: packaging>20.9 in /usr/local/lib/python3.10/dist-packages (from shap) (24.1)\n",
            "Requirement already satisfied: slicer==0.0.8 in /usr/local/lib/python3.10/dist-packages (from shap) (0.0.8)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from shap) (0.60.0)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from shap) (3.1.0)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->shap) (0.43.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->shap) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->shap) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->shap) (2024.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->shap) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->shap) (3.5.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->shap) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import requests\n",
        "import urllib.request\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "from moviepy.editor import ImageSequenceClip\n",
        "import glob\n",
        "import shap\n",
        "import requests\n",
        "from io import BytesIO\n",
        "\n",
        "\n",
        "# Load a pre-trained InceptionV3 model from TensorFlow Hub\n",
        "base_model = tf.keras.applications.InceptionV3(include_top=False, weights='imagenet')\n",
        "base_model.trainable = False\n",
        "\n",
        "# Define a list of layers to target for feature visualization\n",
        "layer_names = ['mixed3', 'mixed5', 'mixed7']\n",
        "\n",
        "# Define the model that outputs the activation values for the target layers\n",
        "dream_model = tf.keras.Model(inputs=base_model.input, outputs=[base_model.get_layer(name).output for name in layer_names])\n",
        "\n",
        "# Define the target layer index for DeepDream\n",
        "target_layer_index = layer_names.index('mixed3')  # Change the target layer here if desired\n",
        "\n",
        "# Define the regularization strength for total variation regularization\n",
        "tv_reg_strength = 1e-6  # Adjust this value to control the smoothness\n",
        "\n",
        "def guided_backprop(model, image, target_layer_index):\n",
        "    with tf.GradientTape() as tape:\n",
        "        tape.watch(image)\n",
        "        layer_activations = model(image)\n",
        "        target_activation = layer_activations[target_layer_index]\n",
        "        loss = tf.math.reduce_mean(target_activation)\n",
        "\n",
        "    grads = tape.gradient(loss, image)\n",
        "    return grads\n",
        "\n",
        "def compute_feature_importance(model, image, target_layer_index):\n",
        "    with tf.GradientTape() as tape:\n",
        "        tape.watch(image)\n",
        "        output = model(image)[target_layer_index]\n",
        "        loss = tf.reduce_mean(output)\n",
        "    gradients = tape.gradient(loss, image)\n",
        "    feature_importance = tf.abs(gradients)\n",
        "    return feature_importance\n",
        "\n",
        "def compute_saliency_map(model, image, target_layer_index):\n",
        "    with tf.GradientTape() as tape:\n",
        "        tape.watch(image)\n",
        "        layer_activations = model(image)\n",
        "        target_activation = layer_activations[target_layer_index]\n",
        "        loss = tf.reduce_mean(target_activation)\n",
        "    grads = tape.gradient(loss, image)\n",
        "    saliency_map = tf.reduce_max(tf.abs(grads), axis=-1, keepdims=True)\n",
        "    saliency_map = tf.repeat(saliency_map, repeats=3, axis=-1)\n",
        "    return saliency_map\n",
        "\n",
        "def guided_grad_cam(model, image, target_layer_index):\n",
        "    cam = calculate_cam(model, image, target_layer_index)\n",
        "    guided_bp = guided_backprop(model, image, target_layer_index)\n",
        "\n",
        "    # Resize CAM to match the input image size\n",
        "    cam_resized = tf.image.resize(cam[..., tf.newaxis], (image.shape[1], image.shape[2]))\n",
        "\n",
        "    # Expand dimensions to match guided backpropagation output\n",
        "    cam_resized = tf.expand_dims(cam_resized, axis=0)\n",
        "    cam_resized = tf.repeat(cam_resized, repeats=3, axis=-1)\n",
        "\n",
        "    guided_grad_cam = cam_resized * guided_bp\n",
        "    return guided_grad_cam\n",
        "\n",
        "def adaptive_step_size(step, initial_step_size, decay_rate=0.9):\n",
        "    return initial_step_size * (decay_rate ** step)\n",
        "\n",
        "def deepdream(img, model, steps, initial_step_size, target_layer_index):\n",
        "    img = tf.convert_to_tensor(img)\n",
        "    for step in range(steps):\n",
        "        with tf.GradientTape() as tape:\n",
        "            tape.watch(img)\n",
        "            loss = tf.reduce_mean(model(img)[target_layer_index])\n",
        "\n",
        "        grads = tape.gradient(loss, img)\n",
        "        grads = tf.math.l2_normalize(grads)\n",
        "\n",
        "        feature_importance = compute_feature_importance(model, img, target_layer_index)\n",
        "        feature_importance = tf.image.resize(feature_importance, (img.shape[1], img.shape[2]))\n",
        "        feature_importance = tf.math.l2_normalize(feature_importance)\n",
        "\n",
        "        saliency_map = compute_saliency_map(model, img, target_layer_index)\n",
        "        saliency_map = tf.image.resize(saliency_map, (img.shape[1], img.shape[2]))\n",
        "        saliency_map = tf.math.l2_normalize(saliency_map)\n",
        "\n",
        "        ggcam = guided_grad_cam(model, img, target_layer_index)\n",
        "        ggcam = tf.math.l2_normalize(ggcam)\n",
        "\n",
        "        tv_loss = total_variation_regularization(img)\n",
        "\n",
        "        # Ensure all guidance signals have the same shape as the input image\n",
        "        combined_grads = grads + 0.2 * feature_importance + 0.2 * saliency_map + 0.2 * ggcam\n",
        "        combined_grads = tf.clip_by_value(combined_grads, -1, 1)\n",
        "\n",
        "        step_size = adaptive_step_size(step, initial_step_size)\n",
        "        img = tf.clip_by_value(img + combined_grads * step_size, -1, 1)\n",
        "\n",
        "        print(f\"Step {step}: Loss: {loss}\")\n",
        "\n",
        "        # Capture CAM at intermediate steps\n",
        "        if step % 2 == 0:\n",
        "            img_np = img.numpy()\n",
        "            cam = calculate_cam(model, img, target_layer_index)\n",
        "            cam_img = visualize_cam(cam, img_np.squeeze())\n",
        "            cam_img = (cam_img * 255).astype(np.uint8)\n",
        "            cam_img = Image.fromarray(cam_img)\n",
        "            cam_img.save(f\"cam_output_{step}.png\")\n",
        "            print(f\"CAM visualization saved to cam_output_{step}.png\")\n",
        "\n",
        "    return img.numpy()\n",
        "\n",
        "def total_variation_regularization(img):\n",
        "    \"\"\"Computes the total variation regularization loss for an image.\"\"\"\n",
        "    x_diff = img[:, :, :-1, :] - img[:, :, 1:, :]\n",
        "    y_diff = img[:, :-1, :, :] - img[:, 1:, :, :]\n",
        "    return tf.reduce_sum(tf.abs(x_diff)) + tf.reduce_sum(tf.abs(y_diff))\n",
        "\n",
        "def download_and_preprocess_image(image_url):\n",
        "    \"\"\"Downloads an image from the URL and preprocesses it for DeepDream.\"\"\"\n",
        "    headers = {\n",
        "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
        "    }\n",
        "    response = requests.get(image_url, headers=headers)\n",
        "    response.raise_for_status()\n",
        "    img = Image.open(BytesIO(response.content))\n",
        "    img = img.convert('RGB')\n",
        "    img = img.resize((299, 299), Image.LANCZOS)\n",
        "    img = np.array(img, dtype='float32') / 255.0\n",
        "    img = np.expand_dims(img, axis=0)\n",
        "    return img\n",
        "\n",
        "def calculate_cam(model, img, target_layer_index):\n",
        "    \"\"\"Calculates Class Activation Mapping (CAM).\"\"\"\n",
        "    if not tf.is_tensor(img):\n",
        "        img = tf.convert_to_tensor(img, dtype=tf.float32)\n",
        "    with tf.GradientTape() as tape:\n",
        "        tape.watch(img)\n",
        "        layer_activations = model(img)\n",
        "        target_activation = layer_activations[target_layer_index]\n",
        "        loss = tf.reduce_mean(target_activation)\n",
        "    grads = tape.gradient(loss, target_activation)\n",
        "    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n",
        "    cam = tf.reduce_sum(tf.multiply(pooled_grads, target_activation), axis=-1)\n",
        "    cam = tf.nn.relu(cam)\n",
        "    cam = cam / tf.reduce_max(cam)\n",
        "    return cam[0].numpy()  # Return the first (and only) CAM as a numpy array\n",
        "\n",
        "def visualize_cam(cam, img, alpha=0.5):\n",
        "    \"\"\"Overlays CAM onto the original image.\"\"\"\n",
        "    if tf.is_tensor(cam):\n",
        "        cam = cam.numpy()\n",
        "    if tf.is_tensor(img):\n",
        "        img = img.numpy()\n",
        "    if len(cam.shape) > 2:\n",
        "        cam = np.squeeze(cam)\n",
        "    # Ensure cam is 2D\n",
        "    if len(cam.shape) != 2:\n",
        "        raise ValueError(f\"Expected CAM to be 2D, but got shape {cam.shape}\")\n",
        "    # Resize CAM to match the original image dimensions\n",
        "    cam = cv2.resize(cam, (img.shape[1], img.shape[0]))\n",
        "    # Normalize CAM to [0, 255] and ensure values are within range\n",
        "    cam = np.uint8(np.clip(cam * 255, 0, 255))\n",
        "    # Convert CAM to heatmap\n",
        "    heatmap = cv2.applyColorMap(cam, cv2.COLORMAP_JET)\n",
        "    # Overlay heatmap on the original image\n",
        "    heatmap = np.float32(heatmap) / 255\n",
        "    cam_img = heatmap * alpha + img\n",
        "    # Normalize the overlay image\n",
        "    cam_img = cam_img / np.max(cam_img)\n",
        "    return cam_img\n",
        "\n",
        "\n",
        "def calculate_sparsity(activations, threshold=0.1):\n",
        "    \"\"\"Calculates activation sparsity.\"\"\"\n",
        "    active_neurons = tf.cast(activations > threshold, tf.float32)\n",
        "    sparsity = 1.0 - tf.reduce_mean(active_neurons).numpy()\n",
        "    return sparsity\n",
        "\n",
        "def calculate_diversity(activations):\n",
        "    \"\"\"Calculates activation diversity.\"\"\"\n",
        "    flat_activations = tf.reshape(activations, [activations.shape[0], -1])\n",
        "    similarity = tf.matmul(flat_activations, flat_activations, transpose_b=True)\n",
        "    diversity = 1 - tf.reduce_mean(similarity)\n",
        "    return diversity.numpy()\n",
        "\n",
        "def calculate_deepdream_score(model, img, target_layer_index):\n",
        "    \"\"\"Calculates DeepDream Score based on activation strength.\"\"\"\n",
        "    layer_activations = model(img)\n",
        "    target_activation = layer_activations[target_layer_index]\n",
        "    score = tf.reduce_mean(tf.abs(target_activation)).numpy()\n",
        "    return score\n",
        "\n",
        "def run_deepdream_multiscale(image_url, num_octaves=3, octave_scale=1.4, steps=10, initial_step_size=1.0):\n",
        "    img = download_and_preprocess_image(image_url)\n",
        "    base_shape = img.shape[1:3]\n",
        "    for octave in range(num_octaves):\n",
        "        new_shape = tuple(int(dim * (octave_scale ** octave)) for dim in base_shape)\n",
        "        img = tf.image.resize(img, new_shape)\n",
        "        img = deepdream(img, dream_model, steps=steps, initial_step_size=initial_step_size, target_layer_index=target_layer_index)\n",
        "\n",
        "    dream_img = np.squeeze(img, axis=0)\n",
        "    dream_img = np.clip(dream_img, 0, 1)\n",
        "    dream_img = (dream_img * 255).astype(np.uint8)\n",
        "    dream_img = Image.fromarray(dream_img)\n",
        "\n",
        "    cam = calculate_cam(dream_model, img, target_layer_index)\n",
        "    deepdream_score = calculate_deepdream_score(dream_model, img, target_layer_index)\n",
        "    print(f\"DeepDream Score: {deepdream_score}\")\n",
        "\n",
        "    output_path = \"deepdream_output.png\"\n",
        "    dream_img.save(output_path, format='PNG')\n",
        "    print(f\"DeepDream image saved to {output_path}\")\n",
        "\n",
        "    cam_img = visualize_cam(cam, img.squeeze())\n",
        "    cam_img = (cam_img * 255).astype(np.uint8)\n",
        "    cam_img = Image.fromarray(cam_img)\n",
        "    cam_img.save(\"final_cam_output.png\")\n",
        "    print(\"CAM visualization saved to final_cam_output.png\")\n",
        "\n",
        "    original_activations = dream_model(tf.convert_to_tensor(img))[target_layer_index]\n",
        "    dream_activations = dream_model(tf.convert_to_tensor(img))[target_layer_index]\n",
        "\n",
        "    original_sparsity = calculate_sparsity(original_activations)\n",
        "    dream_sparsity = calculate_sparsity(dream_activations)\n",
        "    print(f\"Original sparsity: {original_sparsity}, DeepDream sparsity: {dream_sparsity}\")\n",
        "\n",
        "    original_diversity = calculate_diversity(original_activations)\n",
        "    dream_diversity = calculate_diversity(dream_activations)\n",
        "    print(f\"Original diversity: {original_diversity}, DeepDream diversity: {dream_diversity}\")\n",
        "\n",
        "    return dream_img\n",
        "\n",
        "def create_target_model(model, target_layer_index):\n",
        "    return tf.keras.Model(inputs=model.inputs, outputs=model.outputs[target_layer_index])\n",
        "\n",
        "target_model = create_target_model(dream_model, target_layer_index)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    image_url = \"https://images.unsplash.com/photo-1494256997604-768d1f608cac?q=80&w=1229&auto=format&fit=crop&ixlib=rb-4.1.0&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D\"\n",
        "    run_deepdream_multiscale(image_url, num_octaves=3, octave_scale=1.4, steps=10, initial_step_size=1.0)\n",
        "\n",
        "    target_size = (299, 299)\n",
        "    image_files = sorted(glob.glob('cam_output_*.png'))\n",
        "    for image_file in image_files:\n",
        "        img = Image.open(image_file)\n",
        "        img = img.resize(target_size, Image.LANCZOS)\n",
        "        img.save(image_file)\n",
        "\n",
        "    clip = ImageSequenceClip(image_files, fps=15)\n",
        "    clip.write_videofile(\"deepdream_cam_video.mp4\")"
      ],
      "metadata": {
        "id": "0RTK9NRtKNqw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "171ce4aa-f1c4-4658-abc8-6ba767655d67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 0: Loss: 0.4329802989959717\n",
            "CAM visualization saved to cam_output_0.png\n",
            "Step 1: Loss: 0.4763002097606659\n",
            "Step 2: Loss: 0.5150402784347534\n",
            "CAM visualization saved to cam_output_2.png\n",
            "Step 3: Loss: 0.5479326248168945\n",
            "Step 4: Loss: 0.5754392147064209\n",
            "CAM visualization saved to cam_output_4.png\n",
            "Step 5: Loss: 0.5989159345626831\n",
            "Step 6: Loss: 0.6188557744026184\n",
            "CAM visualization saved to cam_output_6.png\n",
            "Step 7: Loss: 0.6359995603561401\n",
            "Step 8: Loss: 0.6510312557220459\n",
            "CAM visualization saved to cam_output_8.png\n",
            "Step 9: Loss: 0.6642836928367615\n",
            "Step 0: Loss: 0.46253493428230286\n",
            "CAM visualization saved to cam_output_0.png\n",
            "Step 1: Loss: 0.5002923607826233\n",
            "Step 2: Loss: 0.5295210480690002\n",
            "CAM visualization saved to cam_output_2.png\n",
            "Step 3: Loss: 0.5529636144638062\n",
            "Step 4: Loss: 0.5724548697471619\n",
            "CAM visualization saved to cam_output_4.png\n",
            "Step 5: Loss: 0.5892603993415833\n",
            "Step 6: Loss: 0.6037738919258118\n",
            "CAM visualization saved to cam_output_6.png\n",
            "Step 7: Loss: 0.6165904402732849\n",
            "Step 8: Loss: 0.6278266906738281\n",
            "CAM visualization saved to cam_output_8.png\n",
            "Step 9: Loss: 0.6378857493400574\n",
            "Step 0: Loss: 0.43151208758354187\n",
            "CAM visualization saved to cam_output_0.png\n",
            "Step 1: Loss: 0.4605325758457184\n",
            "Step 2: Loss: 0.48328641057014465\n",
            "CAM visualization saved to cam_output_2.png\n",
            "Step 3: Loss: 0.5020196437835693\n",
            "Step 4: Loss: 0.5179926156997681\n",
            "CAM visualization saved to cam_output_4.png\n",
            "Step 5: Loss: 0.5318002104759216\n",
            "Step 6: Loss: 0.543908953666687\n",
            "CAM visualization saved to cam_output_6.png\n",
            "Step 7: Loss: 0.5545046329498291\n",
            "Step 8: Loss: 0.5637412667274475\n",
            "CAM visualization saved to cam_output_8.png\n",
            "Step 9: Loss: 0.5719141364097595\n",
            "DeepDream Score: 0.5791539549827576\n",
            "DeepDream image saved to deepdream_output.png\n",
            "CAM visualization saved to final_cam_output.png\n",
            "Original sparsity: 0.48415082693099976, DeepDream sparsity: 0.48415082693099976\n",
            "Original diversity: -895751.125, DeepDream diversity: -895751.125\n",
            "Moviepy - Building video deepdream_cam_video.mp4.\n",
            "Moviepy - Writing video deepdream_cam_video.mp4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "                                                  "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready deepdream_cam_video.mp4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qJKNZJ84Khtb"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "VHtOGjQ5XSYq",
        "ourMke7l38RC",
        "rmKC2BH3mXWn"
      ],
      "authorship_tag": "ABX9TyPFdcwqMfS45krZ8ohakTyb",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}